name: Bot Detection
description: "Detect potential bots by analyzing comment similarity. DOI: https://doi.org/10.1145/3387940.3391503"

on:
  workflow_dispatch:
  schedule:
    - cron: "17 3 * * *"  # daily

permissions:
  contents: read
  pull-requests: read
  issues: write

jobs:
  detect-bots:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install requests

      - name: Run bot detection
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          REPO: ${{ github.repository }}
        run: |
          python << 'EOF'
          import os, requests, datetime, itertools, math
          from collections import defaultdict

          TOKEN = os.environ["GH_TOKEN"]
          REPO = os.environ["REPO"]
          HEADERS = {"Authorization": f"Bearer {TOKEN}"}

          DAYS_BACK = 3
          MAX_PR = 200
          MIN_MESSAGES = 25
          EPS = 0.22
          BOT_CLUSTER_THRESHOLD = 10

          cutoff = datetime.datetime.utcnow() - datetime.timedelta(days=DAYS_BACK)

          def gh(url):
              r = requests.get(url, headers=HEADERS)
              r.raise_for_status()
              return r.json()

          # -----------------------------
          # Fetch PRs
          # -----------------------------
          # Fetch PRs and user account ages
          prs = gh(f"https://api.github.com/repos/{REPO}/pulls?state=all&per_page=100")
          user_created_dates = {}  # Cache user account creation dates

          comments_by_author = defaultdict(list)
          comment_details = defaultdict(list)  # Track PR/issue links and timestamps
          new_accounts = []  # Track accounts created recently (< 7 days)

          for pr in prs[:MAX_PR]:
              if datetime.datetime.strptime(pr["updated_at"], "%Y-%m-%dT%H:%M:%SZ") < cutoff:
                  continue

              number = pr["number"]

              issue_comments = gh(f"https://api.github.com/repos/{REPO}/issues/{number}/comments")
              review_comments = gh(f"https://api.github.com/repos/{REPO}/pulls/{number}/comments")

              for c in issue_comments + review_comments:
                  created = datetime.datetime.strptime(c["created_at"], "%Y-%m-%dT%H:%M:%SZ")
                  if created < cutoff:
                      continue
                  user = c.get("user", {})
                  login = user.get("login")
                  if not login:
                      continue
                  
                  # Fetch user account age if not cached
                  if login not in user_created_dates:
                      try:
                          user_info = gh(f"https://api.github.com/users/{login}")
                          if user_info:
                              created_at = datetime.datetime.strptime(
                                  user_info.get("created_at", ""), "%Y-%m-%dT%H:%M:%SZ"
                              )
                              user_created_dates[login] = created_at
                          else:
                              user_created_dates[login] = None
                      except:
                          user_created_dates[login] = None
                  
                  # Check if account is suspiciously new (< 7 days old)
                  account_created = user_created_dates.get(login)
                  if account_created and (cutoff - account_created).days < 7:
                      new_accounts.append((login, (cutoff - account_created).days))
                  
                  comments_by_author[login].append(c["body"])
                  
                  # Track comment details: PR#, URL, and timestamp
                  comment_url = c.get("html_url", f"https://github.com/{REPO}/pull/{number}#comment")
                  comment_details[login].append({
                      "pr": number,
                      "url": comment_url,
                      "timestamp": c["created_at"]
                  })

          # -----------------------------
          # Distance metrics
          # -----------------------------
          def levenshtein(a, b):
              if len(a) < len(b):
                  a, b = b, a
              prev = range(len(b) + 1)
              for i, ca in enumerate(a):
                  curr = [i + 1]
                  for j, cb in enumerate(b):
                      insert = prev[j + 1] + 1
                      delete = curr[j] + 1
                      replace = prev[j] + (ca != cb)
                      curr.append(min(insert, delete, replace))
                  prev = curr
              return prev[-1]

          def jaccard(a, b):
              sa = set(a.split())
              sb = set(b.split())
              if not sa and not sb:
                  return 0
              return 1 - len(sa & sb) / len(sa | sb)

          # -----------------------------
          # DBSCAN (precomputed distance)
          # -----------------------------
          def dbscan(dist_matrix, eps):
              n = len(dist_matrix)
              labels = [-1] * n
              cluster_id = 0

              for i in range(n):
                  if labels[i] != -1:
                      continue

                  neighbors = [j for j in range(n) if dist_matrix[i][j] <= eps]

                  if not neighbors:
                      continue

                  labels[i] = cluster_id
                  seeds = neighbors[:]

                  while seeds:
                      j = seeds.pop()
                      if labels[j] == -1:
                          labels[j] = cluster_id
                          new_neighbors = [
                              k for k in range(n)
                              if dist_matrix[j][k] <= eps
                          ]
                          seeds.extend(new_neighbors)

                  cluster_id += 1

              return labels

          # -----------------------------
          # Analyze authors
          # -----------------------------
          results = []
          suspicious_unknowns = []  # Track accounts that warrant investigation

          for author, messages in comments_by_author.items():
              # Check for new bots: few messages that are all very similar
              if 3 <= len(messages) < MIN_MESSAGES:
                  norm = [m.lower().strip() for m in messages]
                  
                  # Quick check: all identical
                  if len(set(norm)) == 1:
                      results.append((author, 1, len(messages)))
                      continue
                  
                  # Check if messages are very similar (likely AI bot)
                  n = len(norm)
                  if n >= 2:
                      similarity_sum = 0
                      pair_count = 0
                      for i, j in itertools.combinations(range(n), 2):
                          L = levenshtein(norm[i], norm[j]) / max(len(norm[i]), len(norm[j]), 1)
                          J = jaccard(norm[i], norm[j])
                          D = (L + J) / 2
                          similarity_sum += D
                          pair_count += 1
                      
                      # If average distance is very low (< 0.15), messages are nearly identical
                      avg_distance = similarity_sum / pair_count if pair_count > 0 else 0
                      if avg_distance < 0.15:
                          # Cluster into 1 group if all very similar
                          results.append((author, 1, len(messages)))
                      else:
                          # Flag as suspicious unknown: few messages with unusual patterns
                          suspicious_unknowns.append((author, len(messages), avg_distance))
                      continue
              
              if len(messages) < MIN_MESSAGES:
                  continue

              norm = [m.lower().strip() for m in messages]

              n = len(norm)
              dist_matrix = [[0]*n for _ in range(n)]

              for i, j in itertools.combinations(range(n), 2):
                  L = levenshtein(norm[i], norm[j]) / max(len(norm[i]), len(norm[j]))
                  J = jaccard(norm[i], norm[j])
                  D = (L + J) / 2
                  dist_matrix[i][j] = D
                  dist_matrix[j][i] = D

              labels = dbscan(dist_matrix, EPS)
              clusters = len(set(labels))

              results.append((author, clusters, len(messages)))

          # -----------------------------
          # Build report
          # -----------------------------
          today = datetime.datetime.utcnow().strftime("%Y-%m-%d")
          body = f"# Bot Detection Report â€” {today}\n\n"
          body += f"Parameters: eps={EPS}, threshold={BOT_CLUSTER_THRESHOLD}\n\n"

          body += "## Likely Bots\n"
          for author, clusters, count in sorted(results, key=lambda x: x[1]):
              if clusters <= BOT_CLUSTER_THRESHOLD:
                  body += f"- **{author}** â€” clusters={clusters}, messages={count}\n"
                  # Add links to their comments
                  for detail in comment_details.get(author, []):
                      body += f"  - PR #{detail['pr']}: {detail['url']} ({detail['timestamp']})\n"

          body += "\n## Likely Humans\n"
          for author, clusters, count in sorted(results, key=lambda x: -x[1]):
              if clusters > BOT_CLUSTER_THRESHOLD:
                  body += f"- **{author}** â€” clusters={clusters}, messages={count}\n"

          if suspicious_unknowns:
              body += "\n## Suspicious/Unknown Contributors\n"
              body += "Accounts with few messages and unusual patterns (may warrant investigation):\n\n"
              for author, count, avg_dist in sorted(suspicious_unknowns, key=lambda x: x[1]):
                  body += f"- **{author}** â€” messages={count}, similarity={avg_dist:.2f}\n"
                  # Add links to their comments
                  for detail in comment_details.get(author, []):
                      body += f"  - PR #{detail['pr']}: {detail['url']} ({detail['timestamp']})\n"

          if new_accounts:
              body += "\n## ðŸš¨ HIGH RISK: Brand New Accounts (< 7 days old)\n"
              body += "Recently-created accounts often indicate bots, spam accounts, or malicious actors:\n\n"
              # Deduplicate new accounts
              seen_accounts = {}
              for login, days_old in sorted(new_accounts, key=lambda x: x[1]):
                  if login not in seen_accounts:
                      seen_accounts[login] = days_old
              
              for login, days_old in sorted(seen_accounts.items(), key=lambda x: x[1]):
                  body += f"- **{login}** â€” account age: {days_old} days\n"
                  # Add links to their comments
                  for detail in comment_details.get(login, []):
                      body += f"  - PR #{detail['pr']}: {detail['url']} ({detail['timestamp']})\n"

          requests.post(
              f"https://api.github.com/repos/{REPO}/issues",
              headers=HEADERS,
              json={
                  "title": f"Bot Detection Report â€” {today}",
                  "body": body
              }
          )
          EOF
